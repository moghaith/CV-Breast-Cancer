{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fac6a065",
   "metadata": {},
   "source": [
    "### First We start with the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3165e580",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#we will use pytorch for the neural network here\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import os\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc63e22",
   "metadata": {},
   "source": [
    "### Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12aec52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now will process the image to fit the model specs\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) ], \n",
    "    )\n",
    "#load the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affcc19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/7\n",
      "--------------------\n",
      "train Loss: 0.7460 Acc: 0.6889\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_dir = 'Dataset_BUSI_with_GT'\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])]),\n",
    "\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])])\n",
    "}\n",
    "\n",
    "def split_dataset(dataset_path, output_dir='BUSI_split', val_size=0.2):\n",
    "    classes = ['benign', 'malignant', 'normal']\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for cls in classes:\n",
    "        images = list(Path(dataset_path, cls).glob(\"*.png\"))\n",
    "        train_imgs, val_imgs = train_test_split(images, test_size=val_size, random_state=42)\n",
    "        for phase, img_list in zip(['train', 'val'], [train_imgs, val_imgs]):\n",
    "            phase_path = Path(output_dir) / phase / cls\n",
    "            phase_path.mkdir(parents=True, exist_ok=True)\n",
    "            for img in img_list:\n",
    "                shutil.copy(img, phase_path / img.name)\n",
    "\n",
    "split_dataset('Dataset_BUSI_with_GT')\n",
    "\n",
    "data_dir = 'BUSI_split'\n",
    "\n",
    "image_datasets = {\n",
    "    x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n",
    "    for x in ['train', 'val']\n",
    "}\n",
    "dataloaders = {\n",
    "    x: DataLoader(image_datasets[x], batch_size=16, shuffle=True)\n",
    "    for x in ['train', 'val']\n",
    "}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "model = models.resnet18(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 3)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.fc.parameters(), lr=0.001)\n",
    "\n",
    "def train_model(model, criterion, optimizer, num_epochs=5):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\\n{'-'*20}\")\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            print(f\"{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    print(f\"\\nBest val Acc: {best_acc:.4f}\")\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "model = train_model(model, criterion, optimizer, num_epochs=7)\n",
    "torch.save(model.state_dict(), 'resnet18_busi.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3f2dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the confusion matrix \n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    print(cm)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataloaders, dataset_sizes):\n",
    "#here we will evaluate the model on the validation set\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
